---
title: "Working Pollock ML model with Decadal Aggregation"
author: "Max Kuempel, Tzu-Chun Kao, Owen Peterson"
date: "2025-06-2"
output:
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Packages used are biooracler, terra, randomForest, and caret. biooracler and terra allow for direct downloading of bio-ORACLE layers in R and then raster plotting and analysis respectively. randomForest and caret allow for easy random forest modeling in R. caret is a package that allows easy exploration of data and randomforest models. The devtools package is used to install bioracler from its github page.

NOTE: Rtools may be required for biooracler. Download from here: <https://cran.r-project.org/bin/windows/Rtools/>

Use the PackageInstall script to install all necessary packages. 

Load all required packages

```{r Package Library, message=FALSE, warning=FALSE}
#Libraries for general use.  
library(biooracler)
library(terra)

#Used for Random Forest Model.
library(randomForest)
library(caret)

#Used for final plots
library(gridExtra) #Side by side plotting
library(tidyterra) #ggplot and terra integration
library(viridis)
```

**Reading and processing the pollock trawl surveys**

The trawl surveys come in a .csv file named "survey-points-data.csv." It contains the HaulID, Stratum, lat and long of the trawl, depth of trawl, year of trawl and weight catch per unit effort (kg/ha). HaulID allows matching of this data with a greater metadata file containing the exact start and end coorinates of the trawl path, net properties and more.

First it is read into the R environment with the read.csv function. Data for 2000-2010 and 2010-2020 are loaded in seperately (as to match coresponding decadal Bio-ORACLE layers.)

Weight catch per unit effort (kg of fish per hectare trawled, shortened to wtcpue) values above 0 are selected and log transformed.

```{r Reading the Trawl Surveys}
#reads the trawl survey CSV into R as an object survey.points.data. 
survey.points.data <- read.csv("survey-points-data.csv")
#reads subset of survey.points by year

#2000-2010 survey points
x2000Survey <- subset(survey.points.data, (Year >= 2000 & Year <= 2010))
x2000Survey <- subset(x2000Survey, (wtcpue > 0))
hist(x2000Survey$wtcpue,main= "Pre log transformed catches (2000-2010)")
x2000Survey$wtcpue <- log(x2000Survey$wtcpue)
hist(x2000Survey$wtcpue,main= "Log transformed catches (2000-2010)")


x2010Survey <- subset(survey.points.data, (Year >= 2010 & Year <= 2020))
x2010Survey <- subset(x2010Survey, (wtcpue > 0))
hist(x2010Survey$wtcpue,main= "Pre log transformed catches (2010-2020)")
x2010Survey$wtcpue <- log(x2010Survey$wtcpue)
hist(x2010Survey$wtcpue,main= "Log transformed catches (2010-2020)")

#remove full dataset from R enviroment
remove(survey.points.data)
```

**Loading Bio- layers with bioracler**

Bio-ORACLE provides 0.05 degree near global rasters (seam near the anti-meridan). There is a raster for each decade. For training the model, baseline rasters are used.

All layers for a particular property can be explored with the list_layers("property") function. The specific variables for a layer (mean, min, max etc) can be explored with info_layer("layer name").

In order to download layers from bioracler, a set of constraints is defined. Lat and Long values are taken from the extent of the NOAA Survey Trawls as to not download extra data. date is set to January 1st 2000 in order to download the 2000 baseline layer, and January 1st 2010 for 2010-2020.

To ensure each decade downloads the same variables, all variables are defined in one place. Variables are named with a prefix describing the layer, and a suffix: possible suffixes are

-   mean: Average for the decade

-   max: Maximum for the decade

-   min: Minimum for the decade

-   range: Range of values (difference of max and min for a decade).

-   ltmax: Average Highest value for a year in the decade

-   ltmin: Average lowest value for a year in the decade

```{r Specifying Variables}
#all variables start with "sithick_"
SeaIceThicknessVar <- c("sithick_max")
#all variables start with" o2_"
o2Var <- c("o2_range")
#all varialbes start with "thetao_ "
BenthicTempVar <- c("thetao_range","thetao_max","thetao_min")
#all variables start with "sws"
SeaVelVar <- c("sws_max")
#all variables start with "chl_"
ChlorVar <- c("chl_ltmax","chl_ltmin")


```

```{r DefiningTrainingConstraints}
#Training constraints bound in all 2000-2020 trawl data
latitude_train = c(52, 64) 
longitude_train = c(-157, -179.975)
```


```{r Download Function}
#layer list
LayerList <- list(
  SeaIce = "sithick_baseline_2000_2020_depthsurf",
  o2 = "o2_baseline_2000_2018_depthmax",
    BenthicTemp = "thetao_baseline_2000_2019_depthmax",
    SeaVel = "sws_baseline_2000_2019_depthmax",
 Chlor = "chl_baseline_2000_2018_depthsurf"
)

#Var List
VarList <- list(
  SeaIceID = SeaIceThicknessVar,
  o2ID = o2Var,
  BenthicTempID = BenthicTempVar,
  SeaVelID = SeaVelVar,
  ChlorID = ChlorVar
)



biooracle_download <- function(layer, variables, year,  lat, long) {
  time = c(paste0(year, '-01-01T00:00:00Z'),
paste0(year, '-01-01T00:00:00Z'))
  constraints = list(time, lat, long)
  names(constraints) = c("time", "latitude", "longitude")
  
  download_layers(layer, variables = variables,  constraints = constraints)
  
}
```

```{r 2000 Layer Download}
year <- 2000
LayerNames2000 <- c()#list of layer names allows easy creation of training data
#uses lat and lon from lattitude_train and longitude_train
for (i in 1:length(LayerList)) {
 layer = biooracle_download(LayerList[[i]], 
                     VarList[[i]], 
                     year, 
                     latitude_train, 
                     longitude_train)
  assign(paste0(names(LayerList[i]),as.character(year)), layer )
  remove(layer) #gets rid of last duplicate layer
  LayerNames2000 <-
    append(LayerNames2000, 
           paste0(names(LayerList[i]),as.character(year)))
  }
```

```{r 2010 Layer Download}
year <- 2010
LayerNames2010 <- c()
#uses lat and lon from lattitude_train and longitude_train
for (i in 1:length(LayerList)) {
 layer = biooracle_download(LayerList[[i]], 
                     VarList[[i]], 
                     year, 
                     latitude_train, 
                     longitude_train)
  assign(paste0(names(LayerList[i]),as.character(year)), layer )
  remove(layer) #gets rid of last duplicate layer
  LayerNames2010 <-
    append(LayerNames2010, 
           paste0(names(LayerList[i]),as.character(year)))
  }
```

Depth is downloaded separately of other layers as its date in Bio-ORACLE is set to 1970. Same geographic constraints are used

```{r Bathymetry Download}
depthvariables = "bathymetry_mean" #mean depth per cell. 

Depth <- biooracle_download("terrain_characteristics",
                   depthvariables, 
                   1970, 
                   latitude_train, 
                   longitude_train)
```

In order to train a model, Ocean condtions must be matched to wtcpue values through matched latitudes and longitudes. The extract function allows for this. By extracing a vector of all data point coordiantes we can extract Bio-ORACLE raster values at each coordinate and combine into a dataframe for that decade,called MultipleData[start of decade]. For training the model, a version of MultipleData is made with all variable except the lattitudes and longitudes, with both decades included. This is so that the model does not take these into account. Survey trawls are made roughly at the same points but are slightly off year to year. To aggregate the trawl data temporily to the same time scale of the rasters, dplyr is used to group points in multipled data based off of properties and the mean wtcpue is calcualted for points in the same cell. The unique() function is then used to remove duplicate points (will interfere with model training). 

```{r Extracing and Combining data}
#Creates a new matrix of the lattitudes and longitudes of every trawl
Coord2000 <- cbind(x2000Survey$LON,x2000Survey$LAT)
Coord2010 <- cbind(x2010Survey$LON, x2010Survey$LAT) 

#The "extract" function takes in a lat and long and outputs the value of the raster at that point
#by using it on the whole matrix b, it outputs a list in order of the raster values.
#process is repeated for all bio-ORACLE layers before being bound into a single data frame

RastList2000 <- lapply(LayerNames2000,FUN = get)
MultipleData2000 <- lapply(RastList2000, y = Coord2000, 
                           FUN = terra::extract) %>%
                    as.data.frame()%>% mutate(wtcpue = x2000Survey$wtcpue)
  

RastList2010 <- lapply(LayerNames2010,FUN = get)
MultipleData2010 <- lapply(RastList2010, y = Coord2010, 
                           FUN = terra::extract) %>%
                    as.data.frame()%>% mutate(wtcpue = x2010Survey$wtcpue)
#add in depth
DepthData <- rbind(terra::extract(Depth, Coord2000),
                   terra::extract(Depth, Coord2010))

TrainingData <- rbind(MultipleData2000,MultipleData2010)
TrainingData$depth <- DepthData$bathymetry_mean

#Rename columns. Otherwise all column names are preceded with MultipleData. Nicer to read. 
#names(TrainingData)[names(TrainingData) == "MultipleData.thetao_max"] <- "thetao_max"
#names(TrainingData)[names(TrainingData) == "MultipleData.sithick_max"] <- "sithick_max"
#names(TrainingData)[names(TrainingData) == "MultipleData.o2_mean"] <- "o2_mean"
#names(TrainingData)[names(TrainingData) == "MultipleData.sws_max"] <- "current_max"
#names(TrainingData)[names(TrainingData) == "MultipleData.bathymetry_mean"] <- "depth"
#names(TrainingData)[names(TrainingData) == "MultipleData.x2010Survey.wtcpue"] <- "wtcpue"
#names(TrainingData)[names(TrainingData) == "MultipleData.chl_ltmax"] <- "chl_ltmax"

#Displays the first few rows of the data. 
head(TrainingData)



#mutate aggregation of training data
library(tidyverse)
TrainingData <- TrainingData %>%
  group_by(thetao_range, sithick_max,o2_range,depth) %>%
  mutate(
    meanwtcpue = mean(wtcpue)
  )%>%
  mutate(wtcpue = NULL)#removes column


#remove duplicate points
TrainingData <- unique(TrainingData)

#clean up memory
remove(MultipleData2000)
remove(MultipleData2010)
```

**Creating the Random Forest Model**

The model is created in the randomForest package. It is trained off of 70% of the data and tested on the other 30% (randomly assigned). For repeatability, a set seed of 222 is used.

```{r}

set.seed(222)
#Creates a random list of 70% 1 and 30% 2 that is the same length as the training data. 
#Model is trained off all values corresponding to a 1 (70%)
#30% is set aside (assinged a 2)
ind <- sample(2,nrow(TrainingData), replace = TRUE,prob = c(0.7,0.3))
train <- TrainingData[ind==1,]
test <- TrainingData[ind==2,]


library(caret)
tunegrid <-
  expand.grid(
    mtry = c(1:9)
      )
rf <- train(meanwtcpue~. ,
                  data = train ,
            method = "rf",
                   proximity=FALSE, 
                  ntree=500,
                   tuneGrid = tunegrid,
            trControl = trainControl("cv",
            number = 5)
                   )

#Creates a plot of squared error versus number of trees. 
plot(rf$finalModel, main = "Error vs. Trees for Random Forest Model")
print(rf)
print(rf$finalModel)

#linear regression of predicted versus observed values
lmod <- lm(predict(rf, test)~ test$meanwtcpue)
summary(lmod)

#extract the coefficient of observed values from the linear model
coeficient <- as.numeric(lmod$coefficients[2])
coeficient <- round(coeficient, digits = 3)

#plot predicted versus actual values for the whole training data set. 
#add the coeficient of observed values below the chart

plot(exp(predict(rf, test)), exp(test$meanwtcpue), 
     main = "Predicted versus Observed CPUE (2000-2020)", 
     xlab = "Predicted", 
     ylab = "Observed ", 
     sub = paste("Slope of coeficient:", as.character(coeficient),  "(p < 0.05)"),
     col = "darkcyan",
     pch = 19)
abline(lmod)

#Log Transformed
plot((predict(rf, test)), (test$meanwtcpue), 
     main = "Log Predicted versus Log Observed CPUE (2000-2020)", 
     xlab = "Predicted (log transformed)", 
     ylab = "Observed (log transformed)", 
     sub = paste("Slope of coeficient:", as.character(coeficient),  "(p < 0.05)"),
     pch = 19)
abline(lmod)

#feature importance
 
randomForest::varImpPlot(
  rf$finalModel,
  main = "Feature Importance"
)
```

```{r 2010 Prediction}

year <- 2010
latitude_pred <- c(52, 64)
longitude_pred <- c(-179.75, 179.75)
LayerNames2010_pred <- c()
#uses lat bounded by bering sea, and lon for a global swath
#allows rewrapping around antimeridian
for (i in 1:length(LayerList)) {
 layer = biooracle_download(LayerList[[i]], 
                     VarList[[i]], 
                     year, 
                     latitude_pred, 
                     longitude_pred)
  assign(paste0(names(LayerList[i]),as.character(year),
                "_pred"), layer )
  remove(layer) #gets rid of last duplicate layer
  LayerNames2010_pred <-
    append(LayerNames2010_pred, 
           paste0(names(LayerList[i]),as.character(year),"_pred"))
  }
#depth
Depth_pred <- biooracle_download("terrain_characteristics",
                   depthvariables, 
                   1970, 
                   latitude_pred, 
                   longitude_pred)
####################

#Create a new data frame for all the properties used in the model
LayerNames2010_pred <- append(LayerNames2010_pred, "Depth_pred")
RegionData2010 <- lapply(lapply(LayerNames2010_pred, FUN = get),
                         FUN = terra::values)%>%data.frame()%>%
  mutate(depth = bathymetry_mean)%>%mutate(bathymetry_mean = NULL)




#Creates copy of Bio-ORACLE raster then overwrites data with model predictions
#Assigns a new object to one of the Bio-ORACLE layers. Ensures that all spatial information is maintained. 
Pollock2010Predicted <- DO2010

#un-log transform data
values(Pollock2010Predicted)[!is.na(values(Pollock2010Predicted))] <- 
  exp(predict(rf$finalModel, newdata = na.omit(RegionData2010)))

Pollock2010Predicted <- terra::rotate(Pollock2010Predicted)
e <- ext(-200, -150, 52, 64)
Pollock2010Predicted <-crop(Pollock2010Predicted , e)
#Plot the whole prediction map


  colorscheme <- scale_fill_gradient2(
    low = "black",     # Low values
    mid = "blue4",    # Midpoint
    high = "yellow",     # High values
    midpoint = 40,     # Center value
    name = "Pollock CPUE (kg/ha)",
    limits = c(0,300))
  
ggplot() + geom_spatraster(data = Pollock2010Predicted) + 
  labs(title = "2010 Predicted Pollock Weight Catch per Unit effort (kg/ha)")+ 
  colorscheme+
theme_bw()
```

Using the random forest model and 2040 conditions predicted under SSP3-7.0 (A likely climate pathway with some degree of emissions reduction), the above process is repeated. The study range is expanded to include the whole Bering sea. Due to International Date Line dividing the study area and the way that Bio-ORACLE downloads layers, an entire strip of the world is downloaded from 45 to 75 degrees latitude. Bio-ORACLE data has a strip of missing information of 0.5 degrees along the anti-meridian. This makes the movement of the highest quality zones of the fishery hard to visualize.

To better visualize this data for the report, the final raster is exported as a GeoTIFF to be reprojected in ArcGIS Pro.
```{r SelectClimatePathway}
pathway <- "ssp370"
LayerListPred <- #text manipulate layer names to be future pathway IDs
  lapply(LayerList, pattern = "baseline",replacement = pathway, FUN = sub) %>%
  lapply(LayerList, pattern = "2000_2018", 
         replacement = "2020_2100", FUN = gsub)%>%
  lapply(LayerList, pattern = "2000_2019", 
         replacement = "2020_2100", FUN = gsub)%>%
  lapply(LayerList, pattern = "2000_2020", 
         replacement = "2020_2100", FUN = gsub)
LayerListPred

```
```{r PollockPrediction}

year <- 2040
latitude_pred <- c(52, 64)
longitude_pred <- c(-179.975, 179.975)
LayerNamesPred <- c()
#uses lat bounded by bering sea, and lon for a global swath
#allows rewrapping around antimeridian
for (i in 1:length(LayerList)) {
 layer = biooracle_download(LayerListPred[[i]], 
                     VarList[[i]], 
                     year, 
                     latitude_pred, 
                     longitude_pred)
  assign(paste0(names(LayerListPred[i]),as.character(year),
                "_pred"), layer )
  remove(layer) #gets rid of last duplicate layer
  LayerNamesPred <-
    append(LayerNamesPred, 
           paste0(names(LayerList[i]),as.character(year),"_pred"))
  }
#depth
Depth_pred <- biooracle_download("terrain_characteristics",
                   depthvariables, 
                   1970, 
                   latitude_pred, 
                   longitude_pred)
####################

#Create a new data frame for all the properties used in the model
LayerNamesPred <- append(LayerNamesPred, "Depth_pred")
RegionDataPred <- lapply(lapply(LayerNamesPred, FUN = get),
                         FUN = terra::values)%>%data.frame()%>%
  mutate(depth = bathymetry_mean)%>%mutate(bathymetry_mean = NULL)




#Creates copy of Bio-ORACLE raster then overwrites data with model predictions
#Assigns a new object to one of the Bio-ORACLE layers. Ensures that all spatial information is maintained. 
PollockPred <- get(paste0(names(LayerListPred[1]),as.character(year),
                          "_pred"))

#un-log transform data
values(PollockPred)[!is.na(values(PollockPred))] <- 
  exp(predict(rf$finalModel, newdata = na.omit(RegionDataPred)))

PollockPred <- terra::rotate(PollockPred)
e <- ext(-200, -150, 52, 64)
PollockPred <-crop(PollockPred , e)
#Plot the whole prediction map


  colorscheme <- scale_fill_gradient2(
    low = "black",     # Low values
    mid = "blue4",    # Midpoint
    high = "yellow",     # High values
    midpoint = 40,     # Center value
    name = "Pollock CPUE (kg/ha)",
    limits = c(0,300))
  
ggplot() + geom_spatraster(data = PollockPred) + 
  labs(title = paste(as.character(year), "Predicted Pollock Weight Catch per Unit effort (kg/ha)"))+ 
  colorscheme+
theme_bw()
```


Saving the final 2040 Prediction layer. Change saveraster to true and input your desired filename followed by .tif. Writes as a geotiff file to your default save location. Default is false to prevent unwanted downloading of files.

```{r Saving 2040 Layer}
rastername <- file.path(
  paste0("TIF_outputs/Pollock",
         as.character(year),
         "Predicted.tif")) 

writeRaster(PollockPred,filename = rastername, overwrite = TRUE) #writes a new raster

```

Plotting side by side maps of 2010 and 2040 catches. Saves as a png in the figures folder. 
```{r}
  colorscheme <- scale_fill_gradient2(
    low = "black",     # Low values
    mid = "blue4",    # Midpoint
    high = "yellow",     # High values
    midpoint = 40,     # Center value
    name = "Pollock CPUE (kg/ha)",
    limits = c(0,300))
  
gg2010 <- ggplot() + geom_spatraster(data = Pollock2010Predicted)+
  colorscheme + labs(title = "2010 Baseline conditons") + theme_bw()
gg2040 <- ggplot() + geom_spatraster(data = PollockPred)+
  colorscheme + labs(title = paste(year, "Predicted (SSP3-7.0)")) +theme_bw()
#saving image to folder "Figures"
png("Figures/PollockMapSidebySide.png", width = 800, height = 400)
grid.arrange(gg2010, gg2040, nrow = 1)
dev.off()
```

```{r DIffMap}

  colorscheme <- scale_fill_gradient2(
    low = "red",     # Low values
    mid = "white",    # Midpoint
    high = "blue",     # High values
    midpoint = 0,     # Center value
    name = "Pollock CPUE (kg/ha)",
    limits = c(-260,50))
  
ggChange <- ggplot() + geom_spatraster(data = PollockPred - Pollock2010Predicted)+
  colorscheme + labs(title = paste("Change in Predicted CPUE", year,"- 2010)"), caption = "Catch per Unit Effort (CPUE) measured in catch of pollock (kg) per hectare trawled") +theme_bw()
#saving image
png(paste("Figures/PollockMap",year,"-2010.png"), width = 800, height = 400)
ggChange
dev.off()
```

